# -*- coding: utf-8 -*-
"""API_web_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19WE7sdWap1-RoTYQ-0RY-xfA0J3mSmzi
"""

import requests

from bs4 import BeautifulSoup

import requests



# Step 1: Set the base URL

url = "https://api.open-meteo.com/v1/forecast"



# Step 2: Set parameters (example: Toronto)

params = {

    "latitude": 23.8041,

    "longitude": 90.4152,

    "current_weather": True

}



# Step 3: Make the GET request

response = requests.get(url, params=params)



# Step 4: Check response and print

if response.status_code == 200:

    data = response.json()

    weather = data["current_weather"]

    print(f" Weather in Dhaka:")

    print(f"Temperature: {weather['temperature']}¬∞C")

    print(f"Windspeed: {weather['windspeed']} km/h")

    print(f"Time: {weather['time']}")

else:

    print(" Error:", response.status_code)

import requests

import json

import csv

# --- Step 1: API Call ---

url = "https://api.open-meteo.com/v1/forecast"

params = {

    "latitude": 23.8103,     # Dhaka latitude

    "longitude": 90.4125,    # Dhaka longitude

    "current_weather": True

}

response = requests.get(url, params=params)

# --- Step 2: Check and Extract Data ---

if response.status_code == 200:

    data = response.json()

    weather = data["current_weather"]

    print(f" Weather in Dhaka:")

    print(f"Temperature: {weather['temperature']}¬∞C")

    print(f"Windspeed: {weather['windspeed']} km/h")

    print(f"Time: {weather['time']}")

    # --- Step 3a: Save to JSON ---

    with open("weather_dhaka.json", "w") as json_file:

        json.dump(weather, json_file, indent=4)

        print(" Saved to weather_dhaka.json")

else:

    print(" Error fetching weather data:", response.status_code)

import requests

from bs4 import BeautifulSoup

import csv

import json

# Step 1: Fetch the page

url = "http://quotes.toscrape.com"

response = requests.get(url)

soup = BeautifulSoup(response.text, "html.parser")

# Step 2: Extract quotes and authors

quotes_data = []

quotes = soup.find_all("div", class_="quote")

for quote in quotes:

    text = quote.find("span", class_="text").text.strip()

    author = quote.find("small", class_="author").text.strip()

    quotes_data.append({"quote": text, "author": author})





# Step 3: Save to JSON

with open("quotes.json", "w", encoding="utf-8") as json_file:

    json.dump(quotes_data, json_file, indent=4, ensure_ascii=False)

    print(" Saved to quotes.json")

pip install selenium

pip install webdriver-manager

"""**Note:** Before running this code, please review Prothom Alo's terms of service and robots.txt to ensure you are permitted to scrape their website. The structure of websites can change, which might require updating the code."""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time

# Step 1: Setup Chrome (headless = runs without opening browser window)
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # remove this line if you want to see the browser
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# Step 2: Open browser
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Step 3: Go to Prothom Alo Bangladesh section
url = "https://www.prothomalo.com/bangladesh"
driver.get(url)

# Step 4: Wait for headlines to load
time.sleep(5)  # wait for JavaScript to render

# Step 5: Extract headlines
extracted_headlines = []
headline_elements = driver.find_elements(By.CSS_SELECTOR, "h2 a, h3 a")

for elem in headline_elements[:15]:  # first 15 headlines
    headline_text = elem.text.strip()
    headline_link = elem.get_attribute("href")
    if headline_text and headline_link:
        extracted_headlines.append({"headline": headline_text, "link": headline_link})

# Step 6: Print results
if extracted_headlines:
    print("‚úÖ Prothom Alo Bangladesh Headlines:\n")
    for idx, item in enumerate(extracted_headlines, 1):
        print(f"{idx}. {item['headline']}")
        print(f"   üîó {item['link']}\n")
else:
    print("‚ö†Ô∏è No headlines found.")

# Step 7: Close browser
driver.quit()

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Step 1: Choose a fixed date (YYYY-MM-DD format)
fixed_date = "2025-08-29"   # <-- change this to any date you want
url = f"https://www.prothomalo.com/archive/{fixed_date}"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

try:
    response = requests.get(url, headers=headers)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"Error fetching the page: {e}")
    exit()

# Step 2: Parse HTML
soup = BeautifulSoup(response.content, "html.parser")

# Step 3: Extract headlines
extracted_headlines = []

for card in soup.find_all("h2"):  # archive headlines are inside h2 tags
    a_tag = card.find("a")
    if a_tag and a_tag.text.strip():
        headline_text = a_tag.text.strip()
        headline_link = urljoin(url, a_tag.get("href", ""))
        extracted_headlines.append({"headline": headline_text, "link": headline_link})

# Step 4: Print results
if extracted_headlines:
    print(f"‚úÖ Prothom Alo Headlines for {fixed_date}:\n")
    for idx, item in enumerate(extracted_headlines, 1):
        print(f"{idx}. {item['headline']}")
        print(f"   üîó {item['link']}\n")
else:
    print(f"‚ö†Ô∏è No headlines found for {fixed_date}.")